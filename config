    config = {
    "TOTAL_TIMESTEPS": 2e7,
    "TOTAL_TIMESTEPS_DECAY": 1e7+5e6, # will be used for decay functions, in case you want to test for less timesteps and keep decays same
    "NUM_ENVS": 16, # parallel environments
    "MEMORY_WINDOW": 4, # steps of previous episode added in the rnn training horizon
    "NUM_STEPS": 128, # steps per environment in each update
    "EPS_START": 1,
    "EPS_FINISH": 0.05,
    "EPS_DECAY": 0.1, # ratio of total updates
    "NUM_MINIBATCHES": 16, # minibatches per epoch
    "NUM_EPOCHS": 4, # minibatches per epoch
    "NORM_INPUT": False,
    "HIDDEN_SIZE": 256,
    "NUM_LAYERS": 2,
    "NORM_TYPE": "layer_norm", # layer_norm or batch_norm
    "LR": 0.00005,
    "MAX_GRAD_NORM": 10,
    "LR_LINEAR_DECAY": True,
    "REW_SCALE": 1,
    "GAMMA": 0.99,
    "LAMBDA": 0.95,
    "HYP_TUNE": False,
    "ENTITY": "",
    "PROJECT": "equinox_pqn_lru_clean",
    "WANDB_MODE": "online",
    "SEED": 0,
    "NUM_SEEDS": 1,
    "RNN_TYPE": "lru", # type of rnn to use, lru, fart, gilr, gru
    "PARTIAL": True,

    # env specific
    "ENV_NAME": "BattleShipEasy", # should work also for Acrobot-v1 but might need some tuning
    "ENV_KWARGS": {},

    # evaluation
    "TEST_DURING_TRAINING": False ,
    "TEST_INTERVAL": 0.05 ,# in terms of total updates
    "TEST_NUM_ENVS": 128,
    # TEST_NUM_STEPS: 128 # setup automatically with max env timesteps
    "EPS_TEST": 0, # 0 for greedy policy
    }
